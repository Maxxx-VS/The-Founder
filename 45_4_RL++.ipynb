{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMNByh5kng1jTU/mlC+iFmm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Maxxx-VS/The-Founder/blob/master/45_4_RL%2B%2B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip uninstall gym\n",
        "!pip install gymnasium==0.29.1 torch==2.3.0 matplotlib numpy"
      ],
      "metadata": {
        "id": "SXE_KqISwW4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import gymnasium as gym\n",
        "from collections import deque\n",
        "import random\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Определение архитектуры нейронных сетей\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, max_action):\n",
        "        super(Actor, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(state_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, action_dim),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "        self.max_action = max_action\n",
        "\n",
        "    def forward(self, state):\n",
        "        return self.max_action * self.net(state)\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(Critic, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(state_dim + action_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        return self.net(torch.cat([state, action], 1))\n",
        "\n",
        "# Приоритетный буфер воспроизведения опыта\n",
        "class PrioritizedReplayBuffer:\n",
        "    def __init__(self, capacity, alpha=0.6, beta=0.4, beta_increment=0.001):\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.beta_increment = beta_increment\n",
        "        self.capacity = int(capacity)\n",
        "        self.buffer = []\n",
        "        self.priorities = np.zeros(self.capacity)\n",
        "        self.pos = 0\n",
        "        self.max_priority = 1.0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def add(self, transition):\n",
        "        if len(transition) != 5:  # Проверка формата (s, a, r, s', done)\n",
        "            return\n",
        "        if len(self.buffer) < self.capacity:\n",
        "            self.buffer.append(transition)\n",
        "        else:\n",
        "            self.buffer[self.pos] = transition\n",
        "        self.priorities[self.pos] = self.max_priority\n",
        "        self.pos = (self.pos + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        if len(self.buffer) == 0:\n",
        "            return [], [], []\n",
        "        priorities = self.priorities[:len(self.buffer)]\n",
        "        probs = priorities ** self.alpha\n",
        "        probs /= probs.sum()\n",
        "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
        "        weights = (len(self.buffer) * probs[indices]) ** (-self.beta)\n",
        "        weights /= weights.max()\n",
        "        samples = [self.buffer[i] for i in indices]\n",
        "        return samples, indices, np.array(weights, dtype=np.float32)\n",
        "\n",
        "    def update_priorities(self, indices, priorities):\n",
        "        for idx, priority in zip(indices, priorities):\n",
        "            if idx < len(self.buffer):\n",
        "                self.priorities[idx] = priority\n",
        "        self.max_priority = max(self.priorities.max(), self.max_priority)\n",
        "        self.beta = min(1.0, self.beta + self.beta_increment)\n",
        "\n",
        "# Агент TD3 с ансамблем критиков\n",
        "class TD3Agent:\n",
        "    def __init__(self, state_dim, action_dim, max_action, num_critics=2):\n",
        "        self.actor = Actor(state_dim, action_dim, max_action)\n",
        "        self.actor_target = Actor(state_dim, action_dim, max_action)\n",
        "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=3e-4)\n",
        "\n",
        "        self.critics = [Critic(state_dim, action_dim) for _ in range(num_critics)]\n",
        "        self.critics_target = [Critic(state_dim, action_dim) for _ in range(num_critics)]\n",
        "        for c_target, c in zip(self.critics_target, self.critics):\n",
        "            c_target.load_state_dict(c.state_dict())\n",
        "        self.critic_optimizers = [optim.Adam(c.parameters(), lr=3e-4) for c in self.critics]\n",
        "\n",
        "        self.max_action = max_action\n",
        "        self.tau = 0.005\n",
        "        self.gamma = 0.99\n",
        "        self.policy_noise = 0.2\n",
        "        self.noise_clip = 0.5\n",
        "        self.policy_freq = 2\n",
        "        self.total_it = 0\n",
        "\n",
        "    def select_action(self, state):\n",
        "        state = torch.FloatTensor(state).unsqueeze(0)\n",
        "        return self.actor(state).squeeze().detach().numpy()\n",
        "\n",
        "    def update(self, buffer, batch_size=256):\n",
        "        self.total_it += 1\n",
        "        samples, indices, weights = buffer.sample(batch_size)\n",
        "        if len(samples) == 0:\n",
        "            return\n",
        "\n",
        "        # Распаковка данных\n",
        "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
        "        for sample in samples:\n",
        "            states.append(sample[0])\n",
        "            actions.append(sample[1])\n",
        "            rewards.append(sample[2])\n",
        "            next_states.append(sample[3])\n",
        "            dones.append(sample[4])\n",
        "\n",
        "        # Конвертация в тензоры\n",
        "        state = torch.FloatTensor(np.array(states))\n",
        "        action = torch.FloatTensor(np.array(actions))\n",
        "        reward = torch.FloatTensor(np.array(rewards)).unsqueeze(1)\n",
        "        next_state = torch.FloatTensor(np.array(next_states))\n",
        "        done = torch.FloatTensor(np.array(dones)).unsqueeze(1)\n",
        "        weights = torch.FloatTensor(weights).unsqueeze(1)\n",
        "\n",
        "        # Обновление критиков\n",
        "        with torch.no_grad():\n",
        "            noise = (torch.randn_like(action) * self.policy_noise).clamp(-self.noise_clip, self.noise_clip)\n",
        "            next_action = (self.actor_target(next_state) + noise).clamp(-self.max_action, self.max_action)\n",
        "\n",
        "            target_Q1 = self.critics_target[0](next_state, next_action)\n",
        "            target_Q2 = self.critics_target[1](next_state, next_action)\n",
        "            target_Q = torch.min(target_Q1, target_Q2)\n",
        "            target_Q = reward + (1 - done) * self.gamma * target_Q\n",
        "\n",
        "        critic_losses = []\n",
        "        current_Qs = []\n",
        "        for critic, optimizer in zip(self.critics, self.critic_optimizers):\n",
        "            current_Q = critic(state, action)\n",
        "            current_Qs.append(current_Q)\n",
        "            critic_loss = (weights * (current_Q - target_Q).pow(2)).mean()\n",
        "            optimizer.zero_grad()\n",
        "            critic_loss.backward()\n",
        "            optimizer.step()\n",
        "            critic_losses.append(critic_loss.item())\n",
        "\n",
        "        # Обновление актора и целевых сетей\n",
        "        if self.total_it % self.policy_freq == 0:\n",
        "            actor_loss = -self.critics[0](state, self.actor(state)).mean()\n",
        "            self.actor_optimizer.zero_grad()\n",
        "            actor_loss.backward()\n",
        "            self.actor_optimizer.step()\n",
        "\n",
        "            # Мягкое обновление\n",
        "            for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "\n",
        "            for critic, critic_target in zip(self.critics, self.critics_target):\n",
        "                for param, target_param in zip(critic.parameters(), critic_target.parameters()):\n",
        "                    target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "\n",
        "        # Обновление приоритетов\n",
        "        td_errors = (current_Qs[0] - target_Q).abs().squeeze().detach().numpy()\n",
        "        buffer.update_priorities(indices, td_errors + 1e-5)\n",
        "\n",
        "# Функция обучения\n",
        "def train(env_name=\"Pendulum-v1\", episodes=200):\n",
        "    env = gym.make(env_name)\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.shape[0]\n",
        "    max_action = float(env.action_space.high[0])\n",
        "\n",
        "    agent = TD3Agent(state_dim, action_dim, max_action)\n",
        "    buffer = PrioritizedReplayBuffer(capacity=100000)\n",
        "    rewards = []\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state, _ = env.reset()\n",
        "        total_reward = 0\n",
        "        while True:\n",
        "            action = agent.select_action(state)\n",
        "\n",
        "            # Явное преобразование действия в numpy array\n",
        "            action = np.array(action, dtype=np.float32).reshape(-1)  # Важно: (action_dim,)\n",
        "\n",
        "            try:\n",
        "                # Шаг среды\n",
        "                step_result = env.step(action)\n",
        "                print(\"Step result:\", step_result)  # Отладочный вывод\n",
        "                next_state, reward, terminated, truncated, info = step_result\n",
        "            except Exception as e:\n",
        "                print(f\"Ошибка в env.step(): {e}\")\n",
        "                raise\n",
        "\n",
        "            done = terminated or truncated\n",
        "\n",
        "            # Проверка типов данных\n",
        "            print(f\"Type of next_state: {type(next_state)}\")  # Должен быть np.ndarray\n",
        "            print(f\"Type of reward: {type(reward)}\")          # Должен быть float\n",
        "            print(f\"Type of done: {type(done)}\")              # Должен быть bool\n",
        "\n",
        "            # Преобразование в списки\n",
        "            buffer.add((\n",
        "                np.array(state).flatten().tolist(),    # Гарантированно список\n",
        "                action.tolist(),\n",
        "                float(reward),\n",
        "                np.array(next_state).flatten().tolist(),\n",
        "                float(done)\n",
        "            ))\n",
        "\n",
        "            total_reward += reward\n",
        "            state = np.array(next_state).copy()  # Копирование массива\n",
        "\n",
        "            if len(buffer) >= 256:\n",
        "                agent.update(buffer, 256)\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        rewards.append(total_reward)\n",
        "        print(f\"Episode: {episode+1}, Reward: {total_reward:.2f}\")\n",
        "\n",
        "    return rewards\n",
        "\n",
        "# Визуализация результатов\n",
        "def plot_results(rewards):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(rewards, label='TD3 с приоритетным буфером')\n",
        "    plt.xlabel(\"Эпизод\")\n",
        "    plt.ylabel(\"Награда\")\n",
        "    plt.title(\"Кривая обучения\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.savefig(\"learning_curve.png\")\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    rewards = train(episodes=200)\n",
        "    plot_results(rewards)"
      ],
      "metadata": {
        "id": "DEbzuf2wyu8i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}