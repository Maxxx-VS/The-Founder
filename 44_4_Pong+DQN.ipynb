{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNUirDtnGYTgbYc76kBFtvi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Maxxx-VS/The-Founder/blob/master/44_4_Pong%2BDQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0likKqvIq4x"
      },
      "outputs": [],
      "source": [
        "# Установим необходимые нам библиотеки\n",
        "!pip install gym torch numpy opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Импортируем необходимые библиотеки\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import random\n",
        "import os\n",
        "from gym.wrappers import RecordVideo"
      ],
      "metadata": {
        "id": "_p4MJocCJsAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Создание класса нейронной сети\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "RVgkwYUrJtug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Создание класса агента\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.memory = deque(maxlen=10000)\n",
        "        self.model = DQN(state_dim, action_dim)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
        "        self.criterion = nn.MSELoss()\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_dim)\n",
        "        state = torch.FloatTensor(state).unsqueeze(0)\n",
        "        q_values = self.model(state)\n",
        "        return torch.argmax(q_values).item()\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def replay(self, batch_size):\n",
        "        if len(self.memory) < batch_size:\n",
        "            return\n",
        "        batch = random.sample(self.memory, batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "        states = torch.FloatTensor(np.array(states))\n",
        "        actions = torch.LongTensor(actions)\n",
        "        rewards = torch.FloatTensor(rewards)\n",
        "        next_states = torch.FloatTensor(np.array(next_states))\n",
        "        dones = torch.FloatTensor(dones)\n",
        "\n",
        "        current_q = self.model(states).gather(1, actions.unsqueeze(1))\n",
        "        next_q = self.model(next_states).max(1)[0].detach()\n",
        "        target_q = rewards + (1 - dones) * self.gamma * next_q\n",
        "\n",
        "        loss = self.criterion(current_q.squeeze(), target_q)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay"
      ],
      "metadata": {
        "id": "BrYyY4ClJwLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Предобработка состояния\n",
        "def preprocess(state):\n",
        "    state = state[35:195]  # Обрезка изображения\n",
        "    state = state[::2, ::2, 0]  # Уменьшение размера и выбор одного канала\n",
        "    state[state == 144] = 0  # Удаление фона\n",
        "    state[state == 109] = 0  # Удаление фона\n",
        "    state[state != 0] = 1  # Установка значения 1 для всех объектов\n",
        "    return state.astype(np.float).ravel()  # Преобразование в одномерный массив"
      ],
      "metadata": {
        "id": "DFNCpswrJy0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Создание среды\n",
        "env = gym.make('Pong-v0')\n",
        "state_dim = 80 * 80  # Размер состояния (черно-белое изображение 80x80)\n",
        "action_dim = env.action_space.n"
      ],
      "metadata": {
        "id": "cTVRJiT4KcLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Создание агента\n",
        "agent = DQNAgent(state_dim, action_dim)"
      ],
      "metadata": {
        "id": "j9rGiSkDKdvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Параметры обучения\n",
        "episodes = 1000\n",
        "batch_size = 32\n",
        "best_reward = -float('inf')"
      ],
      "metadata": {
        "id": "lPOkB5vdKgBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Папка для сохранения видео\n",
        "video_dir = \"pong_videos\"\n",
        "os.makedirs(video_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "tup40PnPKhRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Обучение\n",
        "for episode in range(episodes):\n",
        "    state = env.reset()\n",
        "    state = preprocess(state)\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        action = agent.act(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        next_state = preprocess(next_state)\n",
        "        agent.remember(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "        if done:\n",
        "            print(f\"Episode: {episode}, Total Reward: {total_reward}, Epsilon: {agent.epsilon}\")\n",
        "            break\n",
        "\n",
        "        agent.replay(batch_size)\n",
        "\n",
        "    # Сохранение видео лучшего эпизода\n",
        "    if total_reward > best_reward:\n",
        "        best_reward = total_reward\n",
        "        env = RecordVideo(env, video_dir, episode_trigger=lambda x: True)\n",
        "        env.reset()\n",
        "        state = preprocess(env.reset())\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = agent.act(state)\n",
        "            next_state, _, done, _ = env.step(action)\n",
        "            state = preprocess(next_state)\n",
        "        env.close()"
      ],
      "metadata": {
        "id": "avtLsYu4J1W_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}